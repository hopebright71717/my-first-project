import asyncio
import os
import re
import requests
from playwright.async_api import async_playwright

# âœ… å„²å­˜æ ¹ç›®éŒ„ï¼ˆå¯ä¿®æ”¹ï¼‰
BASE_DIR = r"C:/Users/hopeb/Documents/éœ²å¤©API-å°ˆæ¡ˆ/ç©å…·æ§.BBæ§/CO2æ‰‹æ§"

# âœ… å•†å“ç¶²å€åˆ—è¡¨ï¼ˆä¾†è‡ªä½ æä¾›çš„ 200 ç­†ï¼‰
URL_LIST = [
    "https://www.bcsw.com.tw/product/%20UMGSMOS17G?category_sn=1594",
    "https://www.bcsw.com.tw/product/20ASG-16090?category_sn=1594",
    "https://www.bcsw.com.tw/product/ASG-16724?category_sn=1594",
    "https://www.bcsw.com.tw/product/ASG-19592?category_sn=1594",
    "https://www.bcsw.com.tw/product/ASG-19593?category_sn=1594",
    "https://www.bcsw.com.tw/product/ASG-19923?category_sn=1594",
    "https://www.bcsw.com.tw/product/ASG-19924?category_sn=1594",
    "https://www.bcsw.com.tw/product/ASG715BK?category_sn=1594",
    "https://www.bcsw.com.tw/product/E5000145?category_sn=1594",
    "https://www.bcsw.com.tw/product/EMGARK17BK?category_sn=1594",
    # ï¼ˆè€å¸«åªè²¼äº†å‰10ç­†ï¼Œå¯¦éš›ç‰ˆæœ¬ä½ è²¼ä¸Šå®Œæ•´çš„200ç­†ç¶²å€ï¼‰
]

# ğŸ”§ å„²å­˜æ–‡å­—æª”
def save_text(path, content):
    with open(path, "w", encoding="utf-8") as f:
        f.write(content.strip())

# ğŸ”§ å„²å­˜åœ–ç‰‡ï¼ˆä¸å£“ç¸®ï¼‰
def download_image(url, path):
    try:
        if url.startswith("//"):
            url = "https:" + url
        elif url.startswith("/"):
            url = "https://www.bcsw.com.tw" + url
        response = requests.get(url)
        if response.status_code == 200:
            with open(path, "wb") as f:
                f.write(response.content)
    except Exception as e:
        print(f"âš ï¸ ä¸‹è¼‰åœ–ç‰‡å¤±æ•—ï¼š{url}\néŒ¯èª¤ï¼š{e}")

# âœ… å–®ä¸€å•†å“é æ“·å–
async def scrape_bcs_product(page, url):
    try:
        await page.goto(url, timeout=30000)
        await page.wait_for_timeout(1000)

        # å•†å“åç¨±
        await page.wait_for_selector(".product_name.name h1", timeout=5000)
        title = await page.inner_text(".product_name.name h1")
        title = title.strip().replace("ã€BCSã€‘", "ã€ç”²æ­¦ã€‘")

        # å»ºç«‹è³‡æ–™å¤¾
        folder_name = re.sub(r'[\\/:*?"<>|]', '', title)
        folder_path = os.path.join(BASE_DIR, folder_name)
        img_dir = os.path.join(folder_path, "åœ–ç‰‡")
        os.makedirs(img_dir, exist_ok=True)

        # å•†å“ç·¨è™Ÿ & åƒ¹æ ¼
        sn = await page.inner_text(".data-item.number .context")
        price = await page.inner_text("#product_price3_id .digital")

        save_text(os.path.join(folder_path, "å•†å“åç¨±.txt"), title)
        save_text(os.path.join(folder_path, "å•†å“ç·¨è™Ÿ.txt"), sn)
        save_text(os.path.join(folder_path, "å•†å“åƒ¹æ ¼.txt"), price)

        # ä¸»åœ–
        main_img_url = await page.get_attribute("img[data-zoom-image]", "data-zoom-image")
        if main_img_url:
            download_image(main_img_url, os.path.join(img_dir, "ä¸»åœ–.jpg"))

        # å•†å“ä»‹ç´¹ï¼ˆHTMLæ ¼å¼ï¼‰
        await page.click("#product_module_0")
        await page.wait_for_selector("#ajax_box_product_context_id", timeout=5000)
        await page.wait_for_timeout(1000)
        desc_html = await page.inner_html("#ajax_box_product_context_id")
        save_text(os.path.join(folder_path, "å•†å“ä»‹ç´¹.html"), desc_html)

        # å•†å“ä»‹ç´¹åœ–ç‰‡
        img_urls = re.findall(r'<img[^>]+src="([^"]+)"', desc_html)
        for i, img_url in enumerate(img_urls):
            download_image(img_url, os.path.join(img_dir, f"intro_{i+1}.jpg"))

        print(f"âœ… æ“·å–å®Œæˆï¼š{title}")

    except Exception as e:
        print(f"âŒ æ“·å–å¤±æ•—ï¼š{url}\néŒ¯èª¤è¨Šæ¯ï¼š{e}")

# âœ… æ‰¹æ¬¡çˆ¬èŸ²ä»»å‹™
async def scrape_all_products():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        for i, url in enumerate(URL_LIST, 1):
            print(f"\nğŸš€ ç¬¬ {i} ç­†ï¼šé–‹å§‹æ“·å– {url}")
            await scrape_bcs_product(page, url)
            await page.wait_for_timeout(3000)  # ç­‰å¾…3ç§’å†ç¹¼çºŒ

        await browser.close()
        print("\nğŸ‰ æ‰€æœ‰å•†å“æ“·å–å®Œç•¢ï¼")

# âœ… ç¨‹å¼é€²å…¥é»
if __name__ == "__main__":
    asyncio.run(scrape_all_products())
